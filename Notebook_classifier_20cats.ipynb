{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7RLucVUwjNj"
      },
      "source": [
        "# Cat Breed Classifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade wandb"
      ],
      "metadata": {
        "id": "jfJoas7B_j77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99754cad-cee4-4a2c-caba-7834fd377d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint"
      ],
      "metadata": {
        "id": "JN_DQyGa_oQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Ed0YJwN1AA37",
        "outputId": "63273e1c-85c7-4bb9-80bb-3950b6e9b6a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "configs={\n",
        "        \"layer_1\": 512,\n",
        "        \"activation_1\": \"relu\",\n",
        "        \"layer_2\": 256,\n",
        "        \"activation_2\": \"relu\",\n",
        "        \"dropout1\": 0.4,\n",
        "        \"dropout2\": 0.3,\n",
        "        \"layer_3\": 10,\n",
        "        \"activation_3\": \"softmax\",\n",
        "        \"regularization\": 0.001,\n",
        "        \"optimizer\": \"adam\",\n",
        "        \"loss\": \"sparse_categorical_crossentropy\",\n",
        "        \"metric\": \"accuracy\",\n",
        "        \"epoch\": 50,\n",
        "        \"batch_size\": 128\n",
        "    }"
      ],
      "metadata": {
        "id": "E5sAG7VE5k7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a run, tracking hyperparameters\n",
        "wandb.init(\n",
        "    project=\"catbreed_classifier_10\",\n",
        "    config=configs,\n",
        "    reinit=True\n",
        ")\n",
        "\n",
        "# [optional] use wandb.config as your config\n",
        "config = wandb.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192,
          "referenced_widgets": [
            "1b7cc002239147a49fe2d126ef6ad99e",
            "bb0b98041f674897acb321b9685dbb04",
            "aa054c397cf94457bb8ce41ab3d7e496",
            "39b3ce627a944e0a8b359c36a553c200",
            "e1c5a98a140f4fc09ba23c8ac2579c0b",
            "5d83684bbe1346dfb6682ada8b247562",
            "673ef3d83cea4f5faa4be07912299278",
            "9c73a582badc495c8b8fa3d286dba17e"
          ]
        },
        "id": "7LodDCQCAHg6",
        "outputId": "0f1deaf5-4125-4514-aaa4-d98be4b53add"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:eeiky28r) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b7cc002239147a49fe2d126ef6ad99e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">eternal-totem-4</strong> at: <a href='https://wandb.ai/coztomate/catbreed_classifier_10/runs/eeiky28r' target=\"_blank\">https://wandb.ai/coztomate/catbreed_classifier_10/runs/eeiky28r</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20231220_195502-eeiky28r/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:eeiky28r). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231220_195750-77bba26i</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/coztomate/catbreed_classifier_10/runs/77bba26i' target=\"_blank\">cool-cherry-5</a></strong> to <a href='https://wandb.ai/coztomate/catbreed_classifier_10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/coztomate/catbreed_classifier_10' target=\"_blank\">https://wandb.ai/coztomate/catbreed_classifier_10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/coztomate/catbreed_classifier_10/runs/77bba26i' target=\"_blank\">https://wandb.ai/coztomate/catbreed_classifier_10/runs/77bba26i</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to Drive\n"
      ],
      "metadata": {
        "id": "rDFWcWC9_qL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#assessing files from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2rUBOx2xmO-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f4bccc-c677-43aa-c375-57873a6cbf7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/catbreedclassifier\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLtx7k5xxSqH",
        "outputId": "e2759abf-137b-422b-ac88-b2a93a7bf3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images_cats  preprocessed_images.hdf5  Webscraping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count my images"
      ],
      "metadata": {
        "id": "0Kinp_HRWsQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "source_dir = \"/content/drive/My Drive/catbreedclassifier/images_cats\"\n",
        "\n",
        "# Initialize a dictionary to hold the count of images in each subfolder\n",
        "image_counts = {}\n",
        "\n",
        "# Initialize a variable to hold the total count\n",
        "total_images = 0\n",
        "\n",
        "# Iterate over each subfolder\n",
        "for breed_folder in os.listdir(source_dir):\n",
        "    breed_path = os.path.join(source_dir, breed_folder)\n",
        "    if os.path.isdir(breed_path):\n",
        "        # Count the number of files in the subfolder\n",
        "        num_images = len([name for name in os.listdir(breed_path) if os.path.isfile(os.path.join(breed_path, name))])\n",
        "        image_counts[breed_folder] = num_images\n",
        "        total_images += num_images\n",
        "\n",
        "# Print the count for each subfolder\n",
        "for breed, count in image_counts.items():\n",
        "    print(f\"{breed}: {count} images\")\n",
        "\n",
        "# Print the total count\n",
        "print(f\"Total images: {total_images}\")\n"
      ],
      "metadata": {
        "id": "az77d2PGp4Sb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ed14ce-14e9-4dd6-be38-4cb8f637d1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abyssinian: 289 images\n",
            "American Bobtail shorthair: 351 images\n",
            "American Bobtail longhair: 365 images\n",
            "American Curl shorthair: 352 images\n",
            "American Curl longhair: 341 images\n",
            "American Shorthair: 335 images\n",
            "American Wirehair: 493 images\n",
            "Anatoli: 437 images\n",
            "Aphrodite's Giant shorthair: 391 images\n",
            "Aphrodite's Giant longhair: 387 images\n",
            "Arabian Mau: 374 images\n",
            "Asian: 351 images\n",
            "Australian Mist: 407 images\n",
            "Bengal: 364 images\n",
            "Bombay: 370 images\n",
            "Brazilian Shorthair: 374 images\n",
            "British Shorthair: 316 images\n",
            "British Longhair: 347 images\n",
            "Burmese: 385 images\n",
            "Burmilla: 359 images\n",
            "Burmilla longhair: 373 images\n",
            "Celtic Shorthair: 386 images\n",
            "Ceylon: 456 images\n",
            "Chartreux: 328 images\n",
            "Chausie: 376 images\n",
            "Chinese Li Hau: 416 images\n",
            "Classicat: 383 images\n",
            "Colourpoint: 350 images\n",
            "Colourpoint Shorthair: 367 images\n",
            "Cornish Rex: 358 images\n",
            "Cymric: 380 images\n",
            "Devon Rex: 331 images\n",
            "Deutsch Langhaar: 360 images\n",
            "Don Sphynx: 335 images\n",
            "Egyptian Mau: 333 images\n",
            "Exotic Shorthair: 334 images\n",
            "Foreign White shorthair: 358 images\n",
            "Foreign White longhair: 325 images\n",
            "German Rex: 412 images\n",
            "Household pet: 371 images\n",
            "Havana: 329 images\n",
            "Highland Fold: 403 images\n",
            "Highland Straight: 350 images\n",
            "Highlander shorthair: 374 images\n",
            "Highlander longhair: 345 images\n",
            "Japanese Bobtail shorthair: 355 images\n",
            "Japanese Bobtail longhair: 357 images\n",
            "Kanaani: 461 images\n",
            "Karelian Bobtail shorthair: 383 images\n",
            "Karelian Bobtail longhair: 395 images\n",
            "Korat: 381 images\n",
            "Kurilian Bobtail shorthair: 388 images\n",
            "Kurilian Bobtail longhair: 393 images\n",
            "LaPerm shorthair: 353 images\n",
            "LaPerm longhair: 381 images\n",
            "Lykoy: 459 images\n",
            "Maine Coon: 313 images\n",
            "Mandalay: 421 images\n",
            "Manx: 364 images\n",
            "Mekong Bobtail: 391 images\n",
            "Minskin: 392 images\n",
            "Munchkin shorthair: 298 images\n",
            "Munchkin longhair: 319 images\n",
            "Ocicat: 363 images\n",
            "Ojos Azulesshorthair: 358 images\n",
            "Ojos Azules longhair: 370 images\n",
            "Oriental (Semi-) Longhair: 317 images\n",
            "Oriental Shorthair: 264 images\n",
            "Original Longhair: 351 images\n",
            "Persian: 377 images\n",
            "Pixiebob shorthair: 356 images\n",
            "Pixiebob longhair: 360 images\n",
            "Ragamuffin: 301 images\n",
            "Ragdoll: 337 images\n",
            "Russian: 416 images\n",
            "Russian Blue: 330 images\n",
            "Sacred Birman: 346 images\n",
            "Savannah: 329 images\n",
            "Scottish Fold: 352 images\n",
            "Scottish Straight: 370 images\n",
            "Selkirk Rex longhair: 304 images\n",
            "Serengeti: 388 images\n",
            "Siamese: 381 images\n",
            "Siberian cat: 351 images\n",
            "Singapura: 353 images\n",
            "Snowshoe: 363 images\n",
            "Sokoke: 417 images\n",
            "Somali: 322 images\n",
            "Sphynx: 374 images\n",
            "Thai: 380 images\n",
            "Tiffanie: 367 images\n",
            "Tonkanese shorthair: 363 images\n",
            "Tonkanese longhair: 367 images\n",
            "Toybob: 481 images\n",
            "Toyger: 343 images\n",
            "Turkish Angora: 324 images\n",
            "Turkish Van: 356 images\n",
            "Turkish Vankedisi: 375 images\n",
            "Ural Rex shorthair: 406 images\n",
            "Ural Rex longhair: 388 images\n",
            "York: 393 images\n",
            "Total images: 37018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQWaa65swjNn"
      },
      "source": [
        "## Preparing the Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3oVfIx_wjNn"
      },
      "source": [
        "In order for the images to be ready to be used in the models, we have to put them through a pre-processing phase. This includes reshaping them to (384, 384, 3) tensors as it is the recommended shape for the InceptionV3 model input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onLW6NCAwjNn"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.applications import mobilenet_v2\n",
        "\n",
        "#oads an image from a given path, converts it to RGB,\n",
        "#resizes or pads it to a specified size (default 224x224 for MobileNetV2),\n",
        "#and then applies MobileNetV2-specific preprocessing\n",
        "def load_and_preprocess_image(file_path, desired_size=224):\n",
        "    with Image.open(file_path) as img:\n",
        "        img = img.convert('RGB')  # Convert to RGB if not already\n",
        "        # Check if image needs padding\n",
        "        if img.size[0] < desired_size or img.size[1] < desired_size:\n",
        "            img = pad_image(img, desired_size)\n",
        "        else:\n",
        "            img = img.resize((desired_size, desired_size), Image.Resampling.LANCZOS)\n",
        "\n",
        "        img = np.array(img)\n",
        "        img = mobilenet_v2.preprocess_input(img)  # Preprocess for MobileNetV2\n",
        "        return img\n",
        "\n",
        "#helper function used by load_and_preprocess_image to add padding to images\n",
        "#that are smaller than the desired size\n",
        "def pad_image(image, desired_size):\n",
        "    old_size = image.size  # old_size is in (width, height) format\n",
        "    delta_w = desired_size - old_size[0]\n",
        "    delta_h = desired_size - old_size[1]\n",
        "    padding = (delta_w // 2, delta_h // 2, delta_w - (delta_w // 2), delta_h - (delta_h // 2))\n",
        "    return ImageOps.expand(image, padding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Augmentation"
      ],
      "metadata": {
        "id": "HYjP4icOvKKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "def augment_image(image):\n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "        tf.keras.layers.RandomFlip(\"horizontal\"),  # Random horizontal and vertical flip\n",
        "        #tf.keras.layers.RandomBrightness(0.3),  # Random brightness adjustment\n",
        "        #tf.keras.layers.RandomContrast((0.8, 1.2)),  # Random contrast adjustment\n",
        "        #tf.keras.layers.RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)),  # Random zoom\n",
        "    ])\n",
        "    return data_augmentation(image)  # Apply the augmentation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m3s06yU6vM-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Data Generator"
      ],
      "metadata": {
        "id": "0Et7DMQUQn1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#This generator function is designed to yield batches of preprocessed images and their corresponding labels.\n",
        "#It shuffles the file paths and labels at the start of each epoch, then iterates over the dataset in batches,\n",
        "#loading and preprocessing each image in the batch.\n",
        "def image_generator(file_paths, labels, batch_size):\n",
        "    while True:  # Loop forever so the generator never terminates\n",
        "        # Shuffle file paths and labels at the beginning of each epoch\n",
        "        indices = np.arange(len(file_paths))\n",
        "        np.random.shuffle(indices)\n",
        "        file_paths = np.array(file_paths)[indices]\n",
        "        labels = np.array(labels)[indices]\n",
        "\n",
        "        for i in range(0, len(file_paths), batch_size):\n",
        "            batch_paths = file_paths[i:i + batch_size]\n",
        "            batch_labels = labels[i:i + batch_size]\n",
        "\n",
        "            images = []\n",
        "            for path in batch_paths:\n",
        "                # Load and preprocess each image\n",
        "                images.append(load_and_preprocess_image(path))\n",
        "\n",
        "            yield np.array(images), np.array(batch_labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "UWfMisBwQnK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare File Paths and Labels"
      ],
      "metadata": {
        "id": "DWJX1cotQvI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "source_dir = \"/content/drive/My Drive/catbreedclassifier/images_cats\"\n",
        "\n",
        "# Define the 10 specific subfolders you want to include\n",
        "specific_subfolders = ['Ragdoll', 'Maine Coon', 'Exotic Shorthair', 'Persian', 'Devon Rex',\n",
        "                       'British Shorthair', 'Abyssinian', 'American Shorthair', 'Scottish Fold', 'Sphynx']\n",
        "\n",
        "# Generate file paths and labels\n",
        "file_paths = []\n",
        "labels = []\n",
        "\n",
        "for breed_folder in specific_subfolders:\n",
        "    breed_path = os.path.join(source_dir, breed_folder)\n",
        "    all_files = os.listdir(breed_path)\n",
        "    all_files.sort()  # Sort the files to maintain consistency\n",
        "    selected_files = all_files[:200]  # Select only the first 100 files\n",
        "\n",
        "    for filename in selected_files:\n",
        "        file_path = os.path.join(breed_path, filename)\n",
        "        file_paths.append(file_path)\n",
        "        labels.append(breed_folder)\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "le.fit(labels)\n",
        "encoded_labels = le.transform(labels)\n",
        "\n",
        "# Split file paths and labels into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    file_paths, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels)\n"
      ],
      "metadata": {
        "id": "pMfuSK5MQujG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKzzm1GjcSJG",
        "outputId": "b49827f6-5801-4238-d657-0862238a7466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define and Compile the Model"
      ],
      "metadata": {
        "id": "1cOF7NKuRgwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Define ReduceLROnPlateau callback\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',  # Metric to be monitored\n",
        "    factor=0.1,          # Factor by which the learning rate will be reduced. new_lr = lr * factor\n",
        "    patience=5           # Number of epochs with no improvement after which learning rate will be reduced.\n",
        ")\n",
        "\n",
        "# Define EarlyStopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor the validation loss\n",
        "    patience=7,         # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "XfUL2v1y2t5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# loading pretrained model\n",
        "base_net = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=(224, 224, 3),\n",
        "    alpha=1.0,\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    pooling='max'\n",
        ")\n",
        "\n",
        "base_net.trainable = False\n",
        "\n",
        "# build a model\n",
        "model = models.Sequential([\n",
        "    base_net,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(config.layer_1, activation=config.activation_1, kernel_regularizer=regularizers.l2(config.regularization)),\n",
        "    layers.Dropout(config.dropout1),\n",
        "    layers.Dense(config.layer_2, activation=config.activation_2, kernel_regularizer=regularizers.l2(config.regularization)),\n",
        "    layers.Dropout(config.dropout2),\n",
        "    layers.Dense(config.layer_3, activation=config.activation_3)\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=config.optimizer,\n",
        "              loss=config.loss,\n",
        "              metrics=[config.metric]\n",
        "              )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-cARUfLRf_g",
        "outputId": "5ce8a06c-8be4-46e0-8c3b-29655d926395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " mobilenetv2_1.00_224 (Func  (None, 1280)              2257984   \n",
            " tional)                                                         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1280)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               655872    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3047754 (11.63 MB)\n",
            "Trainable params: 789770 (3.01 MB)\n",
            "Non-trainable params: 2257984 (8.61 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Datasets and Train Model"
      ],
      "metadata": {
        "id": "Y6pLlT53Qzfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "batch_size = config.batch_size\n",
        "\n",
        "train_gen = image_generator(X_train, y_train, batch_size)\n",
        "val_gen = image_generator(X_val, y_val, batch_size)\n",
        "\n",
        "steps_per_epoch = len(X_train) // batch_size\n",
        "validation_steps = len(X_val) // batch_size\n",
        "\n",
        "# Define a ModelCheckpoint callback with SavedModel format\n",
        "checkpoint_path = \"models/checkpoint-{epoch:02d}\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_path, save_weights_only=True, save_format='tf', verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=config.epoch,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[\n",
        "        WandbCallback(),  # Automatically logs metrics and uploads model checkpoints\n",
        "        checkpoint,       # Saves model checkpoints locally\n",
        "        early_stopping,   # Early stopping to prevent overfitting\n",
        "        reduce_lr         # Reduce learning rate when a metric has stopped improving\n",
        "    ]\n",
        ")\n",
        "\n",
        "# [optional] finish the wandb run, necessary in notebooks\n",
        "wandb.finish()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7MLmrK5QzRc",
        "outputId": "fd98d759-7a95-41a1-c2f7-d81f58a418ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 5.4750 - accuracy: 0.2207 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For training accuracy\n",
        "best_training_accuracy = max(history.history['accuracy'])\n",
        "\n",
        "# For validation accuracy\n",
        "best_validation_accuracy = max(history.history['val_accuracy'])\n",
        "\n",
        "print(f\"Best training accuracy: {best_training_accuracy}\")\n",
        "print(f\"Best validation accuracy: {best_validation_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CkbyhFuxmiu",
        "outputId": "d3d3c552-37b1-4da4-9a45-d5dd9b29aeee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best training accuracy: 0.9910714030265808\n",
            "Best validation accuracy: 0.8671875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LHdUSSMZxm_X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b7cc002239147a49fe2d126ef6ad99e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb0b98041f674897acb321b9685dbb04",
              "IPY_MODEL_aa054c397cf94457bb8ce41ab3d7e496"
            ],
            "layout": "IPY_MODEL_39b3ce627a944e0a8b359c36a553c200"
          }
        },
        "bb0b98041f674897acb321b9685dbb04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1c5a98a140f4fc09ba23c8ac2579c0b",
            "placeholder": "​",
            "style": "IPY_MODEL_5d83684bbe1346dfb6682ada8b247562",
            "value": "0.011 MB of 0.011 MB uploaded\r"
          }
        },
        "aa054c397cf94457bb8ce41ab3d7e496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_673ef3d83cea4f5faa4be07912299278",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c73a582badc495c8b8fa3d286dba17e",
            "value": 1
          }
        },
        "39b3ce627a944e0a8b359c36a553c200": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1c5a98a140f4fc09ba23c8ac2579c0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d83684bbe1346dfb6682ada8b247562": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "673ef3d83cea4f5faa4be07912299278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c73a582badc495c8b8fa3d286dba17e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}